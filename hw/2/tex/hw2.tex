\documentclass[]{article}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage{biblatex}
\addbibresource{bib.bib}
%opening
\title{}
\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{More Backpropagation}

\subsection{Backprop through a DAG of modules}

First, we refer to assignment 1 for the partial derivative of the sigmoid function $\sigma(x)$ with respect to the input:

\begin{equation}
	\frac{\partial \sigma(x)}{\partial x} = \frac{\partial}{\partial x}\frac{1}{1+\exp(x)} = \frac{\exp(-x)}{(1+\exp(-x))^2}
\end{equation}

\noindent Now consider $\frac{\partial E}{\partial x_1}$. Note that the max and min layers in the network in question are pointless and so only the sigmoid activations and the sum are necessary to consider. Thus we have:

\begin{equation}
	\frac{\partial E}{\partial x_1} = \frac{\partial E}{\partial y} \frac{\partial y}{\partial x_1}= \frac{\partial E}{\partial y} \frac{\partial (\sigma(x_1) + \sigma(x_2))}{\partial x_1} = \frac{\partial E}{\partial y} \frac{\exp(-x_1)}{(1+\exp(-x_1))^2}
\end{equation}

\noindent The second term in the sum, i.e. $\sigma(x_2)$, has partial derivative $0$ as it does not include $x_1$. The argument is exactly the same for $x_2$:

\begin{equation}
\frac{\partial E}{\partial x_2} = \frac{\partial E}{\partial y} \frac{\partial y}{\partial x_1}= \frac{\partial E}{\partial y} \frac{\partial (\sigma(x_1) + \sigma(x_2))}{\partial x_2} = \frac{\partial E}{\partial y} \frac{\exp(-x_2)}{(1+\exp(-x_2))^2}
\end{equation}

\subsection{Batch Normalization}
\subsubsection{(i)}
For simplicity, we will assume that the inputs are one-dimensional. In the more realistic case of multi dimensional inputs, the Jacobian will have to be computed for each dimension.
By the product rule, we have
\[\dfrac{\partial E}{\partial x} = \dfrac{\partial E}{\partial y}\dfrac{\partial y}{\partial x}\]
For the partial derivatives of the output with respect to the input $\dfrac{\partial y_i}{\partial x_j}$, consider two cases:
\paragraph*{$i\neq j$:}
\begin{align*}
  \dfrac{\partial y_i}{\partial y_j} &= \dfrac{\partial x_i - \frac{1}{n}\sum_{k=1}^nx_k}{\partial x_j}\dfrac{1}{\sigma(x)} -\dfrac{1}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_j}\left(x_i - \frac{1}{n}\sum_{k=1}^n x_k\right) \\
&=  \dfrac{\mathbf{E}(x) - x_i}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_j} -\dfrac{1}{n\sigma(x)}\\
\end{align*}
\paragraph*{$i=j$:} 
In this case, the derivative of the numerator changes:
\begin{align*}
  \dfrac{\partial y_i}{\partial x_i} &= \dfrac{\partial x_i - \frac{1}{n}\sum_{k=1}^nx_k}{\partial x_i}\dfrac{1}{\sigma(x)} -\dfrac{1}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_i}\left(x_i - \frac{1}{n}\sum_{k=1}^n x_k\right) \\
&=  \dfrac{\mathbf{E}(x) - x_i}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_i} +\dfrac{n-1}{n}\dfrac{x_i}{\sigma(x)}\\  
\end{align*}
We still need to compute the derivative of the standard deviation with respect to an element $x_j$:
\begin{align*}
  \dfrac{\partial \sigma(x)}{\partial x_j} &= \dfrac{\partial \sigma(x)}{\partial x_j} \\
                                               &= \dfrac{1}{2\sigma(x)} \dfrac{\partial \frac{1}{n}\sum_{k=1}^n (x_k -\frac{1}{n} \sum_{l=1}^n x_l)^2 }{\partial x_j} \\
                                               &= \dfrac{1}{2\sigma(x)} \frac{2}{n}\left( \frac{n-1}{n}\left(x_j- \frac{1}{n}\sum_{l=1}^n x_l \right) -\frac{1}{n}\sum_{k\neq j} \left(x_k -\frac{1}{n} \sum_{l=1}^n x_l\right)\right) \\
                                               &=  \dfrac{1}{n\sigma(x)} \left( \dfrac{n-1}{n}\left(x_j- \mathbf{E}(x) \right) - \dfrac{1}{n}\sum_{k\neq j} (x_k -\mathbf{E}(x))  \right) \\
&=  \dfrac{1}{n\sigma(x)} \left( x_j-\dfrac{1}{n}\sum_{k=1}^n x_k  \right) \\
&=  \dfrac{x_j - \mathbf{E}(x) }{n\sigma(x)}  \\
\end{align*}
\subsubsection{(ii)}
To include a parameter $\epsilon$ that shifts the mean, the forward pass will be
\[y = \dfrac{x-\mathbf{E}(x)}{\sigma(x)} + \epsilon\]
The Jacobian $\frac{\partial y}{\partial x}$ will remain unchanged as the constant $\epsilon$ disappears when taking derivatives. The partial derivative of $y$ with respect to $\epsilon$ will simply be 1:
\[\dfrac{\partial y}{\partial \epsilon} = 1\]
\section{STL-10: semi-supervised image recognition}
We tried to leverage the unlabeled data in two ways. First we used the \texttt{unsup} package to train a Convolutional PSD Autoencoder \cite{convpsd}. The encoder is a convolution followed by the hyperbolic tangent whereas the decoder is simply a convolution. The idea is to learn filters by finding a sparse encoding of patches of the input images. These filters can later be used to initialize a convnet for supervised training. However our results here were terrible.

Second, we tried to build our own stacked convolutional autoencoders (CAE) from scratch, starting with \autocite{zhao2015stacked}.
\subsection{Visualization}
\subsubsection{Filters}

\subsubsection{t-SNE}
We ran t-SNE on the output of each of the stacked encoders. As one can see from the following three figures, visually identifiable clusters emerge especially from the second and third encoder outputs. Recall that our final model did not use any unsupervised pre-training and so these clusters come from purely supervised training.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, keepaspectratio=true]{enc-1}
	\caption{t-SNE applied to output of first encoder}
	\label{fig:enc-1}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, keepaspectratio=true]{enc-2}
	\caption{t-SNE applied to output of second encoder}
	\label{fig:enc-2}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, keepaspectratio=true]{enc-3}
	\caption{t-SNE applied to output of third encoder}
	\label{fig:enc-3}
\end{figure}
\subsection{Data Augmentation}
We extended the given \texttt{BatchFlip} module to perform the following transformation for every input in training mode:
\begin{itemize}
\item Rotate by a Radians chosen uniformly at random from [-0.2,0.2]
\item Translate the x- and y-coordinates by randomly chosen integers between -9 and 9
\item Add Gaussian Noise with a mean of zero and standard deviation of 0.3 to the U and V components, then divide by 1.3 to ensure the inputs come from a distribution with zero mean and unit variance. 
\end{itemize}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio=true]{augmentation}
  \caption{Six randomly selected input images and 5 exemplary transformations; the original (normalized) images are to the left}
  \label{fig:aug}
\end{figure}
Adding the Augmentation module increased the speed of convergence of the model. It also helped against overfitting and resulted in a train accuracy close to the validation accuracy.
\subsection{Stacked CAEs}
Following loosely the example of several papers \autocite{zhao2015stacked,masci2011stacked}, we built a stacked CAE that was trained to minimize reconstruction error. In \autoref{app:A}, the full model architecture is given verbatim. Our architecture was based on two building blocks:
\begin{itemize}
\item An Encoder Module, consisting of a Convolutional layer, a Spatial-Max-Pooling layer, and Dropout/ReLU/Batch Normalization
\item A Decoder Module, consisting of a Spatial-Max-Unpooling layer, a Convolution, and ReLU/BatchNormalization
\end{itemize}
We started by training a single Encoder/Decoder pair by reconstruction loss. We then step-by-step inserted Encoder/Decoder pairs in between the pre-trained Encoder/Decoder pair. After adding two more pairs the model structure looked as follows:\\
\centerline{INPUT - [ENC A - [ENC B - [ENC C - DEC C] - DEC B] - DEC A] - OUTPUT}
The loss function used was the sum of the reconstruction losses for each Decoder/Encoder Pair.
\[\mathcal{L} = \sum_{i=A,B,C} \|\text{Input(i)} - \text{Output(i)}\|_2 \] 
\subsection{PSD Convolutional Autoencoder}

We also trained a PSD Convolutional Autoencoder. This is inspired by \cite{convpsd} and is available from \texttt{unsup} package in \texttt{Torch} 

\subsection{Supervised Training}

\printbibliography
\appendix
\section{CAE Architecture}\label{app:A}
\begin{verbatim}
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> output]
  (1): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]
    (1): nn.SpatialConvolution(3 -> 64, 3x3, 1,1, 1,1)
    (2): nn.SpatialMaxPooling(2,2,2,2)
    (3): nn.SpatialBatchNormalization
    (4): nn.ReLU
    (5): nn.Dropout(0.500000)
  }
  (2): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> output]
    (1): nn.Sequential {
      [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]
      (1): nn.SpatialConvolution(64 -> 128, 3x3, 1,1, 1,1)
      (2): nn.SpatialMaxPooling(2,2,2,2)
      (3): nn.SpatialBatchNormalization
      (4): nn.ReLU
      (5): nn.Dropout(0.500000)
    }
    (2): nn.Sequential {
      [input -> (1) -> (2) -> output]
      (1): nn.Sequential {
        [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]
        (1): nn.SpatialConvolution(128 -> 256, 3x3, 1,1, 1,1)
        (2): nn.SpatialMaxPooling(2,2,2,2)
        (3): nn.SpatialBatchNormalization
        (4): nn.ReLU
        (5): nn.Dropout(0.500000)
      }
      (2): nn.Sequential {
        [input -> (1) -> (2) -> (3) -> (4) -> output]
        (1): nn.SpatialMaxUnpooling associated to nn.SpatialMaxPooling(2,2,2,2)
        (2): nn.SpatialConvolution(256 -> 128, 3x3, 1,1, 1,1)
        (3): nn.ReLU
        (4): nn.SpatialBatchNormalization
      }
    }
    (3): nn.Sequential {
      [input -> (1) -> (2) -> (3) -> (4) -> output]
      (1): nn.SpatialMaxUnpooling associated to nn.SpatialMaxPooling(2,2,2,2)
      (2): nn.SpatialConvolution(128 -> 64, 3x3, 1,1, 1,1)
      (3): nn.ReLU
      (4): nn.SpatialBatchNormalization
    }
  }
  (3): nn.Sequential {
    [input -> (1) -> (2) -> output]
    (1): nn.SpatialMaxUnpooling associated to nn.SpatialMaxPooling(2,2,2,2)
    (2): nn.SpatialConvolution(64 -> 3, 3x3, 1,1, 1,1)
  }
}
\end{verbatim}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
