\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{url}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{appendix}
\usepackage[backend=biber]{biblatex}
\addbibresource{bib.bib}

%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Assignment 2 Deep Learning ST '16}


\author{
Team Hodge Theaters \\
David Kasofsky  \\
\texttt{dk132@nyu.edu}  \\
\And
Jonay Tr\'enous \\
\texttt{jgt275@nyu.edu}\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle
\section{More Backpropagation}

\subsection{Backprop through a DAG of modules}

First, we refer to assignment 1 for the partial derivative of the sigmoid function $\sigma(x)$ with respect to the input:

\begin{equation}
	\frac{\partial \sigma(x)}{\partial x} = \frac{\partial}{\partial x}\frac{1}{1+\exp(x)} = \frac{\exp(-x)}{(1+\exp(-x))^2}
\end{equation}

\noindent Now consider $\frac{\partial E}{\partial x_1}$. Note that the max and min layers in the network in question are pointless and so only the sigmoid activations and the sum are necessary to consider. Thus we have:

\begin{equation}
	\frac{\partial E}{\partial x_1} = \frac{\partial E}{\partial y} \frac{\partial y}{\partial x_1}= \frac{\partial E}{\partial y} \frac{\partial (\sigma(x_1) + \sigma(x_2))}{\partial x_1} = \frac{\partial E}{\partial y} \frac{\exp(-x_1)}{(1+\exp(-x_1))^2}
\end{equation}

\noindent The second term in the sum, i.e. $\sigma(x_2)$, has partial derivative $0$ as it does not include $x_1$. The argument is exactly the same for $x_2$:

\begin{equation}
\frac{\partial E}{\partial x_2} = \frac{\partial E}{\partial y} \frac{\partial y}{\partial x_1}= \frac{\partial E}{\partial y} \frac{\partial (\sigma(x_1) + \sigma(x_2))}{\partial x_2} = \frac{\partial E}{\partial y} \frac{\exp(-x_2)}{(1+\exp(-x_2))^2}
\end{equation}

\subsection{Batch Normalization}
\subsubsection{(i)}
For simplicity, we will assume that the inputs are one-dimensional. In the more realistic case of multi dimensional inputs, the Jacobian will have to be computed for each dimension.
By the product rule, we have
\[\dfrac{\partial E}{\partial x} = \dfrac{\partial E}{\partial y}\dfrac{\partial y}{\partial x}\]
For the partial derivatives of the output with respect to the input $\dfrac{\partial y_i}{\partial x_j}$, consider two cases:
\paragraph*{$i\neq j$:}
\begin{align*}
  \dfrac{\partial y_i}{\partial y_j} &= \dfrac{\partial x_i - \frac{1}{n}\sum_{k=1}^nx_k}{\partial x_j}\dfrac{1}{\sigma(x)} -\dfrac{1}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_j}\left(x_i - \frac{1}{n}\sum_{k=1}^n x_k\right) \\
&=  \dfrac{\mathbf{E}(x) - x_i}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_j} -\dfrac{1}{n\sigma(x)}\\
\end{align*}
\paragraph*{$i=j$:} 
In this case, the derivative of the numerator changes:
\begin{align*}
  \dfrac{\partial y_i}{\partial x_i} &= \dfrac{\partial x_i - \frac{1}{n}\sum_{k=1}^nx_k}{\partial x_i}\dfrac{1}{\sigma(x)} -\dfrac{1}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_i}\left(x_i - \frac{1}{n}\sum_{k=1}^n x_k\right) \\
&=  \dfrac{\mathbf{E}(x) - x_i}{\sigma^2(x)}\dfrac{\partial \sigma(x)}{\partial x_i} +\dfrac{n-1}{n}\dfrac{x_i}{\sigma(x)}\\  
\end{align*}
We still need to compute the derivative of the standard deviation with respect to an element $x_j$:
\begin{align*}
  \dfrac{\partial \sigma(x)}{\partial x_j} &= \dfrac{\partial \sigma(x)}{\partial x_j} \\
                                               &= \dfrac{1}{2\sigma(x)} \dfrac{\partial \frac{1}{n}\sum_{k=1}^n (x_k -\frac{1}{n} \sum_{l=1}^n x_l)^2 }{\partial x_j} \\
                                               &= \dfrac{1}{2\sigma(x)} \frac{2}{n}\left( \frac{n-1}{n}\left(x_j- \frac{1}{n}\sum_{l=1}^n x_l \right) -\frac{1}{n}\sum_{k\neq j} \left(x_k -\frac{1}{n} \sum_{l=1}^n x_l\right)\right) \\
                                               &=  \dfrac{1}{n\sigma(x)} \left( \dfrac{n-1}{n}\left(x_j- \mathbf{E}(x) \right) - \dfrac{1}{n}\sum_{k\neq j} (x_k -\mathbf{E}(x))  \right) \\
&=  \dfrac{1}{n\sigma(x)} \left( x_j-\dfrac{1}{n}\sum_{k=1}^n x_k  \right) \\
&=  \dfrac{x_j - \mathbf{E}(x) }{n\sigma(x)}  \\
\end{align*}
\subsubsection{(ii)}
To include a parameter $\epsilon$ that shifts the mean, the forward pass will be
\[y = \dfrac{x-\mathbf{E}(x)}{\sigma(x)} + \epsilon\]
The Jacobian $\frac{\partial y}{\partial x}$ will remain unchanged as the constant $\epsilon$ disappears when taking derivatives. The partial derivative of $y$ with respect to $\epsilon$ will simply be 1:
\[\dfrac{\partial y}{\partial \epsilon} = 1\]
\section{STL-10: semi-supervised image recognition}
We tried to leverage the unlabeled data in two ways. First we used the \texttt{unsup} package to train a Convolutional PSD Autoencoder \cite{convpsd}. The encoder is a convolution followed by the hyperbolic tangent whereas the decoder is simply a convolution. The idea is to learn filters by finding a sparse encoding of patches of the input images. These filters can later be used to initialize a convnet for supervised training. However our results here were terrible.

Second, we tried to build our own stacked convolutional autoencoders (CAE) from scratch, starting with \autocite{zhao2015stacked}.
\subsection{Our Model}
Our model consisted of three convolutional 'Encoder' layers, followed by a classifier (full architecture see \autoref{app:B}). The specific architecture followed from our attempt to build a stacked CAE to initialize our convolutional weights (see \autoref{sec:scae}); after this failed we used the same architecture with randomly initialized weights for classification.
\subsection{Visualization}
\subsubsection{Filters}
We included filters from each of the three coders (see \autoref{app:E}). One can see that the filters have some structure, e.g. the first encoder's filters look like the desired edges and color blobs.

\subsubsection{t-SNE}
We ran t-SNE on the output of each of the stacked encoders.  As one can see from the three figuresin \autoref{app:D}, visually identifiable clusters emerge especially from the second and third encoder outputs. Recall that our final model did not use any unsupervised pre-training and so these clusters come from purely supervised training.
\subsection{Data Augmentation}
We extended the given \texttt{BatchFlip} module to an \texttt{Augmentation} module that performed the following transformations for every input in training mode:
\begin{itemize}
\item Rotate by a Radians chosen uniformly at random from [-0.2,0.2]
\item Translate the x- and y-coordinates by randomly chosen integers between -9 and 9
\item Add Gaussian Noise with a mean of zero and standard deviation of 0.3 to the U and V components, then divide by 1.3 to ensure the inputs come from a distribution with zero mean and unit variance. 
\end{itemize}
Adding the Augmentation module rapidly increased the speed of convergence of the model ; it achieved after only 30 iterations an accuracy similar to 100 iterations without augmentations. It also helped against overfitting and resulted in a train accuracy close to the validation accuracy. In \autoref{app:E}, examples of the transformations are given.
\subsection{Stacked CAEs}\label{sec:scae}
Following loosely the example of several papers \autocite{zhao2015stacked,masci2011stacked}, we built a stacked CAE that was trained to minimize reconstruction error. In \autoref{app:A}, the full model architecture is given verbatim. Our architecture was based on two building blocks:
\begin{itemize}
\item An Encoder Module, consisting of a Convolutional layer, a Spatial-Max-Pooling layer, and Dropout/ReLU/Batch Normalization
\item A Decoder Module, consisting of a Spatial-Max-Unpooling layer, a Convolution, and ReLU/BatchNormalization
\end{itemize}
We started by training a single Encoder/Decoder pair by reconstruction loss. We then step-by-step inserted Encoder/Decoder pairs in between the pre-trained Encoder/Decoder pair. After adding two more pairs the model structure looked as follows:\\
\centerline{INPUT $\to$ [ENC A - [ENC B - [ENC C - DEC C] - DEC B] - DEC A] $\to$ OUTPUT}
The loss function used was the sum of the reconstruction losses for each Decoder/Encoder Pair.
\[\mathcal{L} = \sum_{i=A,B,C} \|\text{Input(i)} - \text{Output(i)}\|_2 \] 

\subsection{PSD Convolutional Autoencoder}

We also trained a PSD Convolutional Autoencoder (PSD-CAE). This is inspired by \cite{psdconv} and is available from \texttt{unsup} package available for \texttt{Torch}. The architecture is very simple. The encoder is as follows:
\begin{verbatim}
nn.Sequential {
[input -> (1) -> (2) -> (3) -> output]
(1): nn.SpatialConvolutionMap
(2): nn.Tanh
(3): nn.Diag
}
\end{verbatim}

\noindent The decoder is an \texttt{unsup.SpatialConvFistaL1} which seeks an L1-regularized solution. This encourages the weights of the encoder to be sparse. This is good because the encoder increases the dimension of the input representation and so some regularization is necessary in order to avoid learning trivial maps that memorize the input. This would be not be necessary in a deeper architecture that ultimately reduces the dimension of the input representation.

Ultimately we thought to stack a couple of these, training each layer in a greedy fashion. We would then use the weights from the convolutional layers for initialization in supervised training. However there was a problem in training using the \texttt{unsup.ConvPSD} package: we couldn't get it to run on the GPU. Thus the training was quite slow and so we could not make it through enough epochs.

We trained a single PSD-CAE on $\sim20000$ unlabeled images before using the weights to initialize the first convolutional layer in the given VGG model but the results were not identifiably better than the randomly initialized VGG model.
\subsection{Supervised Training}

\printbibliography
\appendix
\section{Stacked CAE Architecture}\label{app:A}
\begin{verbatim}
nn.Sequential {
  [input -> (1) -> (2) -> (3) -> output]
  (1): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]
    (1): nn.SpatialConvolution(3 -> 64, 3x3, 1,1, 1,1)
    (2): nn.SpatialMaxPooling(2,2,2,2)
    (3): nn.SpatialBatchNormalization
    (4): nn.ReLU
    (5): nn.Dropout(0.500000)
  }
  (2): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> output]
    (1): nn.Sequential {
      [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]
      (1): nn.SpatialConvolution(64 -> 128, 3x3, 1,1, 1,1)
      (2): nn.SpatialMaxPooling(2,2,2,2)
      (3): nn.SpatialBatchNormalization
      (4): nn.ReLU
      (5): nn.Dropout(0.500000)
    }
    (2): nn.Sequential {
      [input -> (1) -> (2) -> output]
      (1): nn.Sequential {
        [input -> (1) -> (2) -> (3) -> (4) -> (5) -> output]
        (1): nn.SpatialConvolution(128 -> 256, 3x3, 1,1, 1,1)
        (2): nn.SpatialMaxPooling(2,2,2,2)
        (3): nn.SpatialBatchNormalization
        (4): nn.ReLU
        (5): nn.Dropout(0.500000)
      }
      (2): nn.Sequential {
        [input -> (1) -> (2) -> (3) -> (4) -> output]
        (1): nn.SpatialMaxUnpooling associated to nn.SpatialMaxPooling(2,2,2,2)
        (2): nn.SpatialConvolution(256 -> 128, 3x3, 1,1, 1,1)
        (3): nn.ReLU
        (4): nn.SpatialBatchNormalization
      }
    }
    (3): nn.Sequential {
      [input -> (1) -> (2) -> (3) -> (4) -> output]
      (1): nn.SpatialMaxUnpooling associated to nn.SpatialMaxPooling(2,2,2,2)
      (2): nn.SpatialConvolution(128 -> 64, 3x3, 1,1, 1,1)
      (3): nn.ReLU
      (4): nn.SpatialBatchNormalization
    }
  }
  (3): nn.Sequential {
    [input -> (1) -> (2) -> output]
    (1): nn.SpatialMaxUnpooling associated to nn.SpatialMaxPooling(2,2,2,2)
    (2): nn.SpatialConvolution(64 -> 3, 3x3, 1,1, 1,1)
  }
}
\end{verbatim}

\section{Kaggle Performance}\label{app:B}

\section{Supervised Model}\label{app:C}



\section{t-SNE}\label{app:D}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, keepaspectratio=true]{enc-1}
	\caption{t-SNE applied to output of first encoder}
	\label{fig:enc-1}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, keepaspectratio=true]{enc-2}
	\caption{t-SNE applied to output of second encoder}
	\label{fig:enc-2}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth, keepaspectratio=true]{enc-3}
	\caption{t-SNE applied to output of third encoder}
	\label{fig:enc-3}
\end{figure}
\section{Augmenattions}\label{app:E}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio=true]{augmentation}
  \caption{Six randomly selected input images and 5 exemplary transformations; the original (normalized) images are to the left}
  \label{fig:aug}
\end{figure}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
